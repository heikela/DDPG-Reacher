{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid requirement: './python'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mhei\\AppData\\Local\\conda\\conda\\envs\\drlnd2\\lib\\site-packages\\pip\\_vendor\\packaging\\requirements.py\", line 93, in __init__\n",
      "    req = REQUIREMENT.parseString(requirement_string)\n",
      "  File \"C:\\Users\\mhei\\AppData\\Local\\conda\\conda\\envs\\drlnd2\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 1654, in parseString\n",
      "    raise exc\n",
      "  File \"C:\\Users\\mhei\\AppData\\Local\\conda\\conda\\envs\\drlnd2\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 1644, in parseString\n",
      "    loc, tokens = self._parse( instring, 0 )\n",
      "  File \"C:\\Users\\mhei\\AppData\\Local\\conda\\conda\\envs\\drlnd2\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 1402, in _parseNoCache\n",
      "    loc,tokens = self.parseImpl( instring, preloc, doActions )\n",
      "  File \"C:\\Users\\mhei\\AppData\\Local\\conda\\conda\\envs\\drlnd2\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 3417, in parseImpl\n",
      "    loc, exprtokens = e._parse( instring, loc, doActions )\n",
      "  File \"C:\\Users\\mhei\\AppData\\Local\\conda\\conda\\envs\\drlnd2\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 1402, in _parseNoCache\n",
      "    loc,tokens = self.parseImpl( instring, preloc, doActions )\n",
      "  File \"C:\\Users\\mhei\\AppData\\Local\\conda\\conda\\envs\\drlnd2\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 3739, in parseImpl\n",
      "    return self.expr._parse( instring, loc, doActions, callPreParse=False )\n",
      "  File \"C:\\Users\\mhei\\AppData\\Local\\conda\\conda\\envs\\drlnd2\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 1402, in _parseNoCache\n",
      "    loc,tokens = self.parseImpl( instring, preloc, doActions )\n",
      "  File \"C:\\Users\\mhei\\AppData\\Local\\conda\\conda\\envs\\drlnd2\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 3400, in parseImpl\n",
      "    loc, resultlist = self.exprs[0]._parse( instring, loc, doActions, callPreParse=False )\n",
      "  File \"C:\\Users\\mhei\\AppData\\Local\\conda\\conda\\envs\\drlnd2\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 1406, in _parseNoCache\n",
      "    loc,tokens = self.parseImpl( instring, preloc, doActions )\n",
      "  File \"C:\\Users\\mhei\\AppData\\Local\\conda\\conda\\envs\\drlnd2\\lib\\site-packages\\pip\\_vendor\\pyparsing.py\", line 2711, in parseImpl\n",
      "    raise ParseException(instring, loc, self.errmsg, self)\n",
      "pip._vendor.pyparsing.ParseException: Expected W:(abcd...) (at char 0), (line:1, col:1)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mhei\\AppData\\Local\\conda\\conda\\envs\\drlnd2\\lib\\site-packages\\pip\\_internal\\req\\constructors.py\", line 253, in install_req_from_line\n",
      "    req = Requirement(req)\n",
      "  File \"C:\\Users\\mhei\\AppData\\Local\\conda\\conda\\envs\\drlnd2\\lib\\site-packages\\pip\\_vendor\\packaging\\requirements.py\", line 96, in __init__\n",
      "    requirement_string[e.loc:e.loc + 8], e.msg\n",
      "pip._vendor.packaging.requirements.InvalidRequirement: Parse error at \"'./python'\": Expected W:(abcd...)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "# from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "env = UnityEnvironment(file_name='data/Reacher_Windows_x86_64/Reacher.exe')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.19999999552965164\n",
      "1001\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "time = 0\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    time += 1\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning to act in the reacher environment with DDPG\n",
    "\n",
    "We will use the [DDPG algorithm](https://arxiv.org/abs/1509.02971) to attempt to learn to act in the environment described in the `README.md`. Below we define the building blocks for this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use essentially the same `ReplayBuffer` class as in the Navigation project, except this time we want to be able to retrieve and set the state of the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = Experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            batch_size (int): size of the training batch to sample\n",
    "        \"\"\"\n",
    "        experiences = random.sample(self.memory, k=batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            'memory': self.memory\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.memory = state_dict['memory']\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a noise process based on the [Ornstein-Uhlenbeck process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckNoise():\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "    \n",
    "    def __init__(self, size, mu=0., theta=0.15, sigma=0.2):\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.copy(self.mu)\n",
    "    \n",
    "    def sample(self):\n",
    "        dmean = self.state - self.mu\n",
    "        dx = -self.theta * dmean + self.sigma * np.random.randn(self.size)\n",
    "        self.state += dx\n",
    "        return self.state\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            'size': self.size,\n",
    "            'mu': self.mu,\n",
    "            'theta': self.theta,\n",
    "            'sigma': self.sigma,\n",
    "            'state': self.state\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.size = state_dict['size']\n",
    "        self.mu = state_dict['mu']\n",
    "        self.theta = state_dict['theta']\n",
    "        self.sigma = state_dict['sigma']\n",
    "        self.state = state_dict['state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a utility function for performing soft updates of network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    target parameters = tau * local parameters + (1 - tau) * target parameters\n",
    "    \"\"\"\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a suitable neural network to act as the actor network in DDPG in this environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork3Layer(nn.Module):\n",
    "    \"\"\"Actor Model for DDPG\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, nh1=64, nh2=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(ActorNetwork3Layer, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.FC1 = nn.Linear(state_size, nh1)\n",
    "        self.FC2 = nn.Linear(nh1, nh2)\n",
    "        self.Output = nn.Linear(nh2, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return F.tanh(self.Output(F.leaky_relu(self.FC2(F.leaky_relu(self.FC1(state))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a suitable neural network to act as the criticnetwork in DDPG in this environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork3Layer(nn.Module):\n",
    "    \"\"\"Critic Model for DDPG\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, nh1=64, nh2=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "        \"\"\"\n",
    "        super(CriticNetwork3Layer, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.FC1 = nn.Linear(state_size + action_size, nh1)\n",
    "        self.FC2 = nn.Linear(nh1, nh2)\n",
    "        self.Output = nn.Linear(nh2, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        return self.Output(F.leaky_relu(self.FC2(F.leaky_relu(self.FC1(torch.cat((state, action), dim=1))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an agent that takes care of most of the work in the DDPG algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on whether Cuda is available, place tensor computations on the GPU or CPU.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def mutate_param(param, grow_factor=2.):\n",
    "    choice = np.random.randint(-1, 1)\n",
    "    if choice == -1:\n",
    "        return param / grow_factor\n",
    "    if choice == 0:\n",
    "        return param\n",
    "    return param * grow_factor\n",
    "    \n",
    "\n",
    "class DdpgAgent():\n",
    "    \"\"\"Interacts with the environment and does most of the work of the DDPG algorightm,\n",
    "    manipulating the relevant actor and critic networks and their target copies.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 buffer_size=5e4,\n",
    "                 steps_between_updates=1,\n",
    "                 batch_size=64,\n",
    "                 actor_lr=3e-4,\n",
    "                 critic_lr=3e-4,\n",
    "                 tau=1e-4,\n",
    "                 gamma=0.99,\n",
    "                 seed=66,\n",
    "                 action_space_limit=1,\n",
    "                 state_size=33,\n",
    "                 action_size=4):\n",
    "        \"\"\"Initialize the DdpgAgent object\"\"\"\n",
    "        self.actor_network_local = ActorNetwork3Layer(state_size, action_size, seed).to(device)\n",
    "        self.critic_network_local = CriticNetwork3Layer(state_size, action_size, seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_network_local.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_network_local.parameters(), lr=critic_lr)\n",
    "        self.actor_lr = actor_lr\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_network_target = ActorNetwork3Layer(state_size, action_size, seed).to(device)\n",
    "        self.critic_network_target = CriticNetwork3Layer(state_size, action_size, seed).to(device)\n",
    "        self.noise = OrnsteinUhlenbeckNoise(action_size)\n",
    "        self.experience = ReplayBuffer(int(buffer_size), seed)\n",
    "        self.steps_between_updates = steps_between_updates\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.t_step = 0\n",
    "        self.action_space_limit = action_space_limit\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"Select action based on state\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.from_numpy(state).float().to(device)\n",
    "            self.actor_network_local.eval()\n",
    "            action = self.actor_network_local(state_tensor).cpu().data.numpy()\n",
    "            self.actor_network_local.train()\n",
    "            action += self.noise.sample()\n",
    "            return np.clip(action, -self.action_space_limit, self.action_space_limit)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Record experience and learn based on observed step\"\"\"\n",
    "        self.experience.add(state, action, reward, next_state, done)\n",
    "        self.t_step += 1\n",
    "        if self.t_step % self.steps_between_updates == 0:\n",
    "            self.t_step = 0\n",
    "            self.learn()\n",
    "    \n",
    "    def learn(self):\n",
    "        if len(self.experience) < self.batch_size:\n",
    "            return\n",
    "        # sample a batch of experience\n",
    "        experiences = self.experience.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        # update critic\n",
    "        # calculate targets\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.actor_network_target(next_states)\n",
    "            Q_targets_next = self.critic_network_target(next_states, next_actions)\n",
    "            Q_targets = Q_targets_next + self.gamma * rewards\n",
    "        Q_predicted = self.critic_network_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_predicted, Q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        # update actor\n",
    "        predicted_actions = self.actor_network_local(states)\n",
    "        actor_loss = -self.critic_network_local(states, predicted_actions).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # soft update targets towards locals\n",
    "        self.soft_update()\n",
    "    \n",
    "    def soft_update(self):\n",
    "        soft_update(self.actor_network_local, self.actor_network_target, self.tau)\n",
    "        soft_update(self.critic_network_local, self.critic_network_target, self.tau)\n",
    "    \n",
    "    def forward_state_dict(self):\n",
    "        return {\n",
    "            'actor': self.actor_network_local.state_dict(),\n",
    "            'critic': self.critic_network_local.state_dict(),\n",
    "            'actor_target': self.actor_network_target.state_dict(),\n",
    "            'critic_target': self.critic_network_target.state_dict(),\n",
    "            'noise': self.noise.state_dict()\n",
    "        }\n",
    "    \n",
    "    def load_forward_state_dict(self, state_dict):\n",
    "        self.actor_network_local.load_state_dict(state_dict['actor'])\n",
    "        self.critic_network_local.load_state_dict(state_dict['critic'])\n",
    "        self.actor_network_target.load_state_dict(state_dict['actor_target'])\n",
    "        self.critic_network_target.load_state_dict(state_dict['critic_target'])\n",
    "        self.noise.load_state_dict(state_dict['noise'])\n",
    "    \n",
    "    def learning_state_dict(self):\n",
    "        return {\n",
    "            'actor_optim_state': self.actor_optimizer.state_dict(),\n",
    "            'critic_optim_state': self.critic_optimizer.state_dict(),\n",
    "            'replay_buffer': self.experience.state_dict()\n",
    "        }\n",
    "    \n",
    "    def load_learning_state_dict(self, state_dict):\n",
    "        self.actor_optimizer.load_state_dict(state_dict['actor_optim_state'])\n",
    "        self.critic_optimizer.load_state_dict(state_dict['critic_optim_state'])\n",
    "        self.experience.load_state_dict(state_dict['replay_buffer'])\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            'forward': self.forward_state_dict(),\n",
    "            'learning': self.learning_state_dict()\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.load_forward_state_dict(state_dict['forward'])\n",
    "        self.load_learning_state_dict(state_dict['learning'])\n",
    "    \n",
    "    def hyperparameter_dict(self):\n",
    "        return {\n",
    "            'actor_lr': self.actor_lr,\n",
    "            'critic_lr': self.critic_lr,\n",
    "            'gamma': self.gamma,\n",
    "            'tau': self.tau\n",
    "        }\n",
    "    \n",
    "    def load_hyperparameter_dict(self, hyperparameter_dict):\n",
    "        for p in self.actor_optimizer.param_groups:\n",
    "            p['lr'] = hyperparameter_dict['actor_lr']\n",
    "        for p in self.critic_optimizer.param_groups:\n",
    "            p['lr'] = hyperparameter_dict['critic_lr']\n",
    "        self.gamma = hyperparameter_dict['gamma']\n",
    "        self.tau = hyperparameter_dict['tau']\n",
    "    \n",
    "    def load_mutated_hyperparameter_dict(self, hyperparameter_dict):\n",
    "        self.load_hyperparameter_dict({\n",
    "            'actor_lr': mutate_param(hyperparameter_dict['actor_lr']),\n",
    "            'critic_lr': mutate_param(hyperparameter_dict['critic_lr']),\n",
    "            'gamma': 1 - mutate_param(1 - hyperparameter_dict['gamma']),\n",
    "            'tau': mutate_param(hyperparameter_dict['tau']),\n",
    "        })\n",
    "    \n",
    "    def full_save_dict(self, scores, agent_name):\n",
    "        return {\n",
    "            'agent_name': agent_name,\n",
    "            'state': self.state_dict(),\n",
    "            'hyperparameters': self.hyperparameter_dict(),\n",
    "            'scores': scores\n",
    "        }\n",
    "    \n",
    "    def load_full_save_dict(self, full_save_dict):\n",
    "        self.load_state_dict(full_save_dict['state'])\n",
    "        self.load_hyperparameter_dict(full_save_dict['hyperparameters'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(agent, env, brain_name,\n",
    "         max_episode=1000, max_t=1100, previous_scores=None,\n",
    "         solution_threshold=30, solution_episodes=100,\n",
    "         checkpoint_episodes=100, agent_name='agent-0'):\n",
    "    \"\"\"Carry out learning based on the DDPG algorithm.\"\"\"\n",
    "    scores = previous_scores if previous_scores else []\n",
    "    scores_window = deque(scores, maxlen=solution_episodes)\n",
    "    for i_episode in range(len(scores) + 1, max_episode+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        for t in range(0, max_t):\n",
    "            action = agent.act(state)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            reward = env_info.rewards[0]\n",
    "            done = env_info.local_done[0]\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % checkpoint_episodes == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            torch.save(agent.full_save_dict(scores, agent_name),\"{}_episode_{}.pth\".format(agent_name, i_episode))\n",
    "        if len(scores_window) >= solution_episodes and np.mean(scores_window)>=solution_threshold:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-solution_episodes, np.mean(scores_window)))\n",
    "            torch.save(agent.full_state_dict(scores, agent_name),\"{}_solved.pth\".format(agent_name))\n",
    "            break\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find good hyperparameters and training results simultaenously with [population based training](https://arxiv.org/abs/1711.09846)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'actor_lr': 0.001, 'critic_lr': 0.001, 'gamma': 0.95, 'tau': 0.0003},\n",
       " {'actor_lr': 0.001, 'critic_lr': 0.001, 'gamma': 0.95, 'tau': 0.0001},\n",
       " {'actor_lr': 0.001, 'critic_lr': 0.001, 'gamma': 0.99, 'tau': 0.0003},\n",
       " {'actor_lr': 0.001, 'critic_lr': 0.001, 'gamma': 0.99, 'tau': 0.0001},\n",
       " {'actor_lr': 0.001, 'critic_lr': 0.0003, 'gamma': 0.95, 'tau': 0.0003},\n",
       " {'actor_lr': 0.001, 'critic_lr': 0.0003, 'gamma': 0.95, 'tau': 0.0001},\n",
       " {'actor_lr': 0.001, 'critic_lr': 0.0003, 'gamma': 0.99, 'tau': 0.0003},\n",
       " {'actor_lr': 0.001, 'critic_lr': 0.0003, 'gamma': 0.99, 'tau': 0.0001},\n",
       " {'actor_lr': 0.0003, 'critic_lr': 0.001, 'gamma': 0.95, 'tau': 0.0003},\n",
       " {'actor_lr': 0.0003, 'critic_lr': 0.001, 'gamma': 0.95, 'tau': 0.0001},\n",
       " {'actor_lr': 0.0003, 'critic_lr': 0.001, 'gamma': 0.99, 'tau': 0.0003},\n",
       " {'actor_lr': 0.0003, 'critic_lr': 0.001, 'gamma': 0.99, 'tau': 0.0001},\n",
       " {'actor_lr': 0.0003, 'critic_lr': 0.0003, 'gamma': 0.95, 'tau': 0.0003},\n",
       " {'actor_lr': 0.0003, 'critic_lr': 0.0003, 'gamma': 0.95, 'tau': 0.0001},\n",
       " {'actor_lr': 0.0003, 'critic_lr': 0.0003, 'gamma': 0.99, 'tau': 0.0003},\n",
       " {'actor_lr': 0.0003, 'critic_lr': 0.0003, 'gamma': 0.99, 'tau': 0.0001}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_hyperparams = [{\n",
    "        'actor_lr': actor_lr,\n",
    "        'critic_lr': critic_lr,\n",
    "        'gamma': gamma,\n",
    "        'tau': tau\n",
    "    }\n",
    "    for actor_lr in [1e-3, 3e-4]\n",
    "    for critic_lr in [1e-3, 3e-4]\n",
    "    for gamma in [0.95, 0.99]\n",
    "    for tau in [3e-4, 1e-4]\n",
    "]\n",
    "starting_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_agents = [\n",
    "    DdpgAgent(actor_lr=p['actor_lr'],\n",
    "              critic_lr=p['critic_lr'],\n",
    "              tau=p['tau'],\n",
    "              gamma=p['gamma'])\n",
    "    for p in starting_hyperparams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_based_training(agents, env, brain_name,\n",
    "                              max_episode=20,\n",
    "                              max_t=1100,\n",
    "                              episodes_between_exploit=20,\n",
    "                              p_threshold = 0.05,\n",
    "                              existing_scores = None,\n",
    "                              checkpoint_episodes = None):\n",
    "    checkpoint_episodes = checkpoint_episodes if checkpoint_episodes else episodes_between_exploits\n",
    "    agent_scores = existing_scores if existing_scores else [[] for a in agents]\n",
    "    episode_i = len(agent_scores[0]) + 1\n",
    "    while episode_i <= max_episode:\n",
    "        episode_i += episodes_between_exploit\n",
    "        for agent_i, agent in enumerate(agents):\n",
    "            prev_t = len(agent_scores[agent_i])\n",
    "            agent_scores[agent_i] = ddpg(\n",
    "                agent, env, brain_name,\n",
    "                max_episode=prev_t + episodes_between_exploit,\n",
    "                previous_scores=agent_scores[agent_i],\n",
    "                agent_name=\"agent_{}\".format(agent_i),\n",
    "                checkpoint_episodes=checkpoint_episodes\n",
    "            )\n",
    "            print(\"\\r\\nAgent: {}, mean return: {:.2f}\".format(agent_i, np.mean(agent_scores[agent_i][-episodes_between_exploit:])))\n",
    "        for target_i, target_agent in enumerate(agents):\n",
    "            # uniformly select a candidate that is not the same as target\n",
    "            candidate_i = np.random.randint(0, len(agents) - 1)\n",
    "            if candidate_i >= target_i:\n",
    "                candidate_i += 1\n",
    "            candidate = agents[candidate_i]\n",
    "            # exploit\n",
    "            candidate_scores = agent_scores[candidate_i][-episodes_between_exploit:]\n",
    "            target_scores = agent_scores[target_i][-episodes_between_exploit:]\n",
    "            exploited = False\n",
    "            if np.mean(candidate_scores) > np.mean(target_scores):\n",
    "                statistic, p = stats.ttest_ind(candidate_scores, target_scores, equal_var=False)\n",
    "                if p < p_threshold:\n",
    "                    print(\"Overwriting agent {} with agent {}, p = {:.2f}\".format(target_i, candidate_i, p))\n",
    "                    agent_name = \"agent_{}\".format(target_i)\n",
    "                    torch.save(target_agent.full_save_dict(agent_scores[target_i], agent_name),\n",
    "                              \"retire_{}_episode_{}.pth\".format(agent_name, len(agent_scores[target_i])))\n",
    "                    exploited = True\n",
    "                    target_agent.load_state_dict(candidate.state_dict())\n",
    "                else:\n",
    "                    print(\"Agent {} performed worse than agent {} but it wasn't significant, p = {:.2f}\".format(target_i, candidate_i, p))\n",
    "            if exploited:\n",
    "                # explore\n",
    "                source_hyperparameters = candidate.hyperparameter_dict()\n",
    "                target_agent.load_mutated_hyperparameter_dict(source_hyperparameters)\n",
    "    return agent_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tAverage Score: 0.15\n",
      "\n",
      "Agent: 0, mean return: 0.15\n",
      "Episode 20\tAverage Score: 0.28\n",
      "\n",
      "Agent: 1, mean return: 0.28\n",
      "Episode 20\tAverage Score: 0.50\n",
      "\n",
      "Agent: 2, mean return: 0.50\n",
      "Episode 20\tAverage Score: 0.09\n",
      "\n",
      "Agent: 3, mean return: 0.09\n",
      "Episode 20\tAverage Score: 1.00\n",
      "\n",
      "Agent: 4, mean return: 1.00\n",
      "Episode 20\tAverage Score: 0.49\n",
      "\n",
      "Agent: 5, mean return: 0.49\n",
      "Episode 20\tAverage Score: 0.59\n",
      "\n",
      "Agent: 6, mean return: 0.59\n",
      "Episode 20\tAverage Score: 0.66\n",
      "\n",
      "Agent: 7, mean return: 0.66\n",
      "Episode 20\tAverage Score: 0.39\n",
      "\n",
      "Agent: 8, mean return: 0.39\n",
      "Episode 20\tAverage Score: 0.78\n",
      "\n",
      "Agent: 9, mean return: 0.78\n",
      "Episode 20\tAverage Score: 0.03\n",
      "\n",
      "Agent: 10, mean return: 0.03\n",
      "Episode 20\tAverage Score: 0.56\n",
      "\n",
      "Agent: 11, mean return: 0.56\n",
      "Episode 20\tAverage Score: 0.79\n",
      "\n",
      "Agent: 12, mean return: 0.79\n",
      "Episode 20\tAverage Score: 0.59\n",
      "\n",
      "Agent: 13, mean return: 0.59\n",
      "Episode 20\tAverage Score: 0.67\n",
      "\n",
      "Agent: 14, mean return: 0.67\n",
      "Episode 20\tAverage Score: 0.29\n",
      "\n",
      "Agent: 15, mean return: 0.29\n",
      "Overwriting agent 0 with agent 13, p = 0.00\n",
      "Overwriting agent 3 with agent 2, p = 0.00\n",
      "Agent 6 performed worse than agent 7 but it wasn't significant, p = 0.63\n",
      "Agent 7 performed worse than agent 4 but it wasn't significant, p = 0.07\n",
      "Overwriting agent 10 with agent 15, p = 0.00\n",
      "Agent 11 performed worse than agent 12 but it wasn't significant, p = 0.16\n",
      "Agent 13 performed worse than agent 7 but it wasn't significant, p = 0.67\n",
      "Episode 40\tAverage Score: 0.18\n",
      "\n",
      "Agent: 0, mean return: 0.21\n",
      "Episode 40\tAverage Score: 0.18\n",
      "\n",
      "Agent: 1, mean return: 0.09\n",
      "Episode 40\tAverage Score: 0.32\n",
      "\n",
      "Agent: 2, mean return: 0.15\n",
      "Episode 40\tAverage Score: 0.13\n",
      "\n",
      "Agent: 3, mean return: 0.16\n",
      "Episode 40\tAverage Score: 0.79\n",
      "\n",
      "Agent: 4, mean return: 0.58\n",
      "Episode 40\tAverage Score: 0.48\n",
      "\n",
      "Agent: 5, mean return: 0.46\n",
      "Episode 40\tAverage Score: 0.73\n",
      "\n",
      "Agent: 6, mean return: 0.86\n",
      "Episode 40\tAverage Score: 0.55\n",
      "\n",
      "Agent: 7, mean return: 0.44\n",
      "Episode 40\tAverage Score: 0.35\n",
      "\n",
      "Agent: 8, mean return: 0.30\n",
      "Episode 40\tAverage Score: 0.60\n",
      "\n",
      "Agent: 9, mean return: 0.42\n",
      "Episode 40\tAverage Score: 0.12\n",
      "\n",
      "Agent: 10, mean return: 0.20\n",
      "Episode 40\tAverage Score: 0.45\n",
      "\n",
      "Agent: 11, mean return: 0.33\n",
      "Episode 40\tAverage Score: 0.51\n",
      "\n",
      "Agent: 12, mean return: 0.24\n",
      "Episode 40\tAverage Score: 0.36\n",
      "\n",
      "Agent: 13, mean return: 0.12\n",
      "Episode 40\tAverage Score: 0.51\n",
      "\n",
      "Agent: 14, mean return: 0.34\n",
      "Episode 40\tAverage Score: 0.25\n",
      "\n",
      "Agent: 15, mean return: 0.22\n",
      "Overwriting agent 0 with agent 4, p = 0.00\n",
      "Overwriting agent 1 with agent 8, p = 0.04\n",
      "Agent 2 performed worse than agent 0 but it wasn't significant, p = 0.48\n",
      "Agent 7 performed worse than agent 5 but it wasn't significant, p = 0.85\n",
      "Agent 13 performed worse than agent 12 but it wasn't significant, p = 0.12\n",
      "Agent 14 performed worse than agent 5 but it wasn't significant, p = 0.43\n",
      "Agent 15 performed worse than agent 11 but it wasn't significant, p = 0.31\n",
      "Episode 60\tAverage Score: 0.42\n",
      "\n",
      "Agent: 0, mean return: 0.90\n",
      "Episode 60\tAverage Score: 0.25\n",
      "\n",
      "Agent: 1, mean return: 0.39\n",
      "Episode 60\tAverage Score: 0.37\n",
      "\n",
      "Agent: 2, mean return: 0.46\n",
      "Episode 60\tAverage Score: 0.29\n",
      "\n",
      "Agent: 3, mean return: 0.63\n",
      "Episode 60\tAverage Score: 0.99\n",
      "\n",
      "Agent: 4, mean return: 1.38\n",
      "Episode 60\tAverage Score: 0.36\n",
      "\n",
      "Agent: 5, mean return: 0.12\n",
      "Episode 60\tAverage Score: 0.85\n",
      "\n",
      "Agent: 6, mean return: 1.09\n",
      "Episode 60\tAverage Score: 0.44\n",
      "\n",
      "Agent: 7, mean return: 0.20\n",
      "Episode 60\tAverage Score: 0.43\n",
      "\n",
      "Agent: 8, mean return: 0.59\n",
      "Episode 60\tAverage Score: 0.49\n",
      "\n",
      "Agent: 9, mean return: 0.28\n",
      "Episode 60\tAverage Score: 0.16\n",
      "\n",
      "Agent: 10, mean return: 0.24\n",
      "Episode 60\tAverage Score: 0.35\n",
      "\n",
      "Agent: 11, mean return: 0.15\n",
      "Episode 60\tAverage Score: 0.52\n",
      "\n",
      "Agent: 12, mean return: 0.52\n",
      "Episode 60\tAverage Score: 0.28\n",
      "\n",
      "Agent: 13, mean return: 0.12\n",
      "Episode 60\tAverage Score: 0.51\n",
      "\n",
      "Agent: 14, mean return: 0.50\n",
      "Episode 60\tAverage Score: 0.21\n",
      "\n",
      "Agent: 15, mean return: 0.12\n",
      "Agent 2 performed worse than agent 14 but it wasn't significant, p = 0.79\n",
      "Overwriting agent 5 with agent 2, p = 0.00\n",
      "Overwriting agent 11 with agent 6, p = 0.00\n",
      "Overwriting agent 13 with agent 12, p = 0.01\n",
      "Overwriting agent 15 with agent 2, p = 0.00\n",
      "Episode 67\tAverage Score: 0.53"
     ]
    }
   ],
   "source": [
    "scores = population_based_training(starting_agents, env, brain_name, max_episode=1000, episodes_between_exploit=20, checkpoint_episodes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_values = [torch.load('agent_{}_episode_60.pth'.format(i)) for i in range(0,16)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = [DdpgAgent() for dict in saved_values]\n",
    "for i, saved_dict in enumerate(saved_values):\n",
    "    agents[i].load_full_save_dict(saved_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [saved_dict['scores'] for saved_dict in saved_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 80\tAverage Score: 0.58\n",
      "\n",
      "Agent: 0, mean return: 1.05\n",
      "Episode 80\tAverage Score: 0.32\n",
      "\n",
      "Agent: 1, mean return: 0.53\n",
      "Episode 80\tAverage Score: 0.47\n",
      "\n",
      "Agent: 2, mean return: 0.77\n",
      "Episode 80\tAverage Score: 0.33\n",
      "\n",
      "Agent: 3, mean return: 0.45\n",
      "Episode 80\tAverage Score: 1.02\n",
      "\n",
      "Agent: 4, mean return: 1.10\n",
      "Episode 80\tAverage Score: 0.31\n",
      "\n",
      "Agent: 5, mean return: 0.17\n",
      "Episode 80\tAverage Score: 0.74\n",
      "\n",
      "Agent: 6, mean return: 0.41\n",
      "Episode 80\tAverage Score: 0.36\n",
      "\n",
      "Agent: 7, mean return: 0.14\n",
      "Episode 80\tAverage Score: 0.45\n",
      "\n",
      "Agent: 8, mean return: 0.50\n",
      "Episode 80\tAverage Score: 0.45\n",
      "\n",
      "Agent: 9, mean return: 0.32\n",
      "Episode 80\tAverage Score: 0.18\n",
      "\n",
      "Agent: 10, mean return: 0.24\n",
      "Episode 80\tAverage Score: 0.28\n",
      "\n",
      "Agent: 11, mean return: 0.06\n",
      "Episode 80\tAverage Score: 0.53\n",
      "\n",
      "Agent: 12, mean return: 0.59\n",
      "Episode 80\tAverage Score: 0.26\n",
      "\n",
      "Agent: 13, mean return: 0.18\n",
      "Episode 80\tAverage Score: 0.67\n",
      "\n",
      "Agent: 14, mean return: 1.17\n",
      "Episode 80\tAverage Score: 0.20\n",
      "\n",
      "Agent: 15, mean return: 0.19\n",
      "Agent 3 performed worse than agent 12 but it wasn't significant, p = 0.50\n",
      "Overwriting agent 5 with agent 8, p = 0.02\n",
      "Agent 7 performed worse than agent 10 but it wasn't significant, p = 0.15\n",
      "Overwriting agent 10 with agent 1, p = 0.03\n",
      "Overwriting agent 11 with agent 9, p = 0.00\n",
      "Overwriting agent 13 with agent 0, p = 0.00\n",
      "Overwriting agent 15 with agent 14, p = 0.00\n",
      "Episode 100\tAverage Score: 0.61\n",
      "\n",
      "Agent: 0, mean return: 0.74\n",
      "Episode 100\tAverage Score: 0.38\n",
      "\n",
      "Agent: 1, mean return: 0.62\n",
      "Episode 100\tAverage Score: 0.39\n",
      "\n",
      "Agent: 2, mean return: 0.05\n",
      "Episode 100\tAverage Score: 0.29\n",
      "\n",
      "Agent: 3, mean return: 0.12\n",
      "Episode 100\tAverage Score: 1.23\n",
      "\n",
      "Agent: 4, mean return: 2.07\n",
      "Episode 100\tAverage Score: 0.28\n",
      "\n",
      "Agent: 5, mean return: 0.16\n",
      "Episode 100\tAverage Score: 0.72\n",
      "\n",
      "Agent: 6, mean return: 0.64\n",
      "Episode 100\tAverage Score: 0.38\n",
      "\n",
      "Agent: 7, mean return: 0.43\n",
      "Episode 100\tAverage Score: 0.42\n",
      "\n",
      "Agent: 8, mean return: 0.31\n",
      "Episode 100\tAverage Score: 0.39\n",
      "\n",
      "Agent: 9, mean return: 0.16\n",
      "Episode 100\tAverage Score: 0.26\n",
      "\n",
      "Agent: 10, mean return: 0.58\n",
      "Episode 100\tAverage Score: 0.27\n",
      "\n",
      "Agent: 11, mean return: 0.22\n",
      "Episode 100\tAverage Score: 0.54\n",
      "\n",
      "Agent: 12, mean return: 0.55\n",
      "Episode 100\tAverage Score: 0.29\n",
      "\n",
      "Agent: 13, mean return: 0.44\n",
      "Episode 100\tAverage Score: 0.61\n",
      "\n",
      "Agent: 14, mean return: 0.37\n",
      "Episode 100\tAverage Score: 0.25\n",
      "\n",
      "Agent: 15, mean return: 0.43\n",
      "Overwriting agent 2 with agent 14, p = 0.03\n",
      "Agent 3 performed worse than agent 14 but it wasn't significant, p = 0.07\n",
      "Agent 7 performed worse than agent 6 but it wasn't significant, p = 0.14\n",
      "Overwriting agent 8 with agent 0, p = 0.01\n",
      "Overwriting agent 9 with agent 13, p = 0.01\n",
      "Agent 11 performed worse than agent 15 but it wasn't significant, p = 0.17\n",
      "Overwriting agent 12 with agent 4, p = 0.02\n",
      "Agent 15 performed worse than agent 6 but it wasn't significant, p = 0.23\n",
      "Episode 120\tAverage Score: 0.69\n",
      "\n",
      "Agent: 0, mean return: 0.54\n",
      "Episode 120\tAverage Score: 0.55\n",
      "\n",
      "Agent: 1, mean return: 1.12\n",
      "Episode 120\tAverage Score: 0.31\n",
      "\n",
      "Agent: 2, mean return: 0.11\n",
      "Episode 120\tAverage Score: 0.32\n",
      "\n",
      "Agent: 3, mean return: 0.24\n",
      "Episode 120\tAverage Score: 1.32\n",
      "\n",
      "Agent: 4, mean return: 1.45\n",
      "Episode 120\tAverage Score: 0.21\n",
      "\n",
      "Agent: 5, mean return: 0.14\n",
      "Episode 120\tAverage Score: 0.78\n",
      "\n",
      "Agent: 6, mean return: 0.91\n",
      "Episode 120\tAverage Score: 0.29\n",
      "\n",
      "Agent: 7, mean return: 0.23\n",
      "Episode 120\tAverage Score: 0.47\n",
      "\n",
      "Agent: 8, mean return: 0.64\n",
      "Episode 120\tAverage Score: 0.38\n",
      "\n",
      "Agent: 9, mean return: 0.74\n",
      "Episode 120\tAverage Score: 0.46\n",
      "\n",
      "Agent: 10, mean return: 1.05\n",
      "Episode 120\tAverage Score: 0.25\n",
      "\n",
      "Agent: 11, mean return: 0.50\n",
      "Episode 120\tAverage Score: 0.73\n",
      "\n",
      "Agent: 12, mean return: 1.77\n",
      "Episode 120\tAverage Score: 0.31\n",
      "\n",
      "Agent: 13, mean return: 0.68\n",
      "Episode 120\tAverage Score: 0.49\n",
      "\n",
      "Agent: 14, mean return: 0.09\n",
      "Episode 120\tAverage Score: 0.27\n",
      "\n",
      "Agent: 15, mean return: 0.39\n",
      "Agent 0 performed worse than agent 13 but it wasn't significant, p = 0.49\n",
      "Agent 2 performed worse than agent 7 but it wasn't significant, p = 0.22\n",
      "Overwriting agent 5 with agent 10, p = 0.00\n",
      "Overwriting agent 6 with agent 4, p = 0.05\n",
      "Agent 10 performed worse than agent 1 but it wasn't significant, p = 0.81\n",
      "Agent 14 performed worse than agent 2 but it wasn't significant, p = 0.80\n",
      "Episode 140\tAverage Score: 0.77\n",
      "\n",
      "Agent: 0, mean return: 0.63\n",
      "Episode 140\tAverage Score: 0.70\n",
      "\n",
      "Agent: 1, mean return: 0.83\n",
      "Episode 140\tAverage Score: 0.35\n",
      "\n",
      "Agent: 2, mean return: 0.35\n",
      "Episode 140\tAverage Score: 0.32\n",
      "\n",
      "Agent: 3, mean return: 0.16\n",
      "Episode 140\tAverage Score: 1.49\n",
      "\n",
      "Agent: 4, mean return: 1.45\n",
      "Episode 140\tAverage Score: 0.30\n",
      "\n",
      "Agent: 5, mean return: 0.93\n",
      "Episode 140\tAverage Score: 0.91\n",
      "\n",
      "Agent: 6, mean return: 1.51\n",
      "Episode 140\tAverage Score: 0.36\n",
      "\n",
      "Agent: 7, mean return: 0.78\n",
      "Episode 140\tAverage Score: 0.53\n",
      "\n",
      "Agent: 8, mean return: 0.63\n",
      "Episode 140\tAverage Score: 0.45\n",
      "\n",
      "Agent: 9, mean return: 0.76\n",
      "Episode 140\tAverage Score: 0.56\n",
      "\n",
      "Agent: 10, mean return: 0.69\n",
      "Episode 140\tAverage Score: 0.36\n",
      "\n",
      "Agent: 11, mean return: 0.85\n",
      "Episode 140\tAverage Score: 0.98\n",
      "\n",
      "Agent: 12, mean return: 1.48\n",
      "Episode 140\tAverage Score: 0.38\n",
      "\n",
      "Agent: 13, mean return: 0.47\n",
      "Episode 140\tAverage Score: 0.49\n",
      "\n",
      "Agent: 14, mean return: 0.31\n",
      "Episode 140\tAverage Score: 0.28\n",
      "\n",
      "Agent: 15, mean return: 0.29\n",
      "Agent 0 performed worse than agent 10 but it wasn't significant, p = 0.77\n",
      "Overwriting agent 2 with agent 9, p = 0.01\n",
      "Overwriting agent 3 with agent 11, p = 0.00\n",
      "Agent 8 performed worse than agent 11 but it wasn't significant, p = 0.23\n",
      "Overwriting agent 9 with agent 12, p = 0.02\n",
      "Agent 13 performed worse than agent 0 but it wasn't significant, p = 0.35\n",
      "Agent 14 performed worse than agent 13 but it wasn't significant, p = 0.27\n",
      "Overwriting agent 15 with agent 10, p = 0.04\n",
      "Episode 160\tAverage Score: 0.77\n",
      "\n",
      "Agent: 0, mean return: 0.87\n",
      "Episode 160\tAverage Score: 0.71\n",
      "\n",
      "Agent: 1, mean return: 0.42\n",
      "Episode 160\tAverage Score: 0.40\n",
      "\n",
      "Agent: 2, mean return: 0.72\n",
      "Episode 160\tAverage Score: 0.34\n",
      "\n",
      "Agent: 3, mean return: 0.72\n",
      "Episode 160\tAverage Score: 1.72\n",
      "\n",
      "Agent: 4, mean return: 2.54\n",
      "Episode 160\tAverage Score: 0.38\n",
      "\n",
      "Agent: 5, mean return: 0.48\n",
      "Episode 160\tAverage Score: 1.13\n",
      "\n",
      "Agent: 6, mean return: 2.18\n",
      "Episode 160\tAverage Score: 0.55\n",
      "\n",
      "Agent: 7, mean return: 1.16\n",
      "Episode 160\tAverage Score: 0.66\n",
      "\n",
      "Agent: 8, mean return: 1.22\n",
      "Episode 160\tAverage Score: 1.01\n",
      "\n",
      "Agent: 9, mean return: 3.07\n",
      "Episode 160\tAverage Score: 0.58\n",
      "\n",
      "Agent: 10, mean return: 0.32\n",
      "Episode 160\tAverage Score: 0.50\n",
      "\n",
      "Agent: 11, mean return: 0.87\n",
      "Episode 160\tAverage Score: 1.54\n",
      "\n",
      "Agent: 12, mean return: 3.29\n",
      "Episode 160\tAverage Score: 0.43\n",
      "\n",
      "Agent: 13, mean return: 0.38\n",
      "Episode 160\tAverage Score: 0.64\n",
      "\n",
      "Agent: 14, mean return: 1.29\n",
      "Episode 160\tAverage Score: 0.31\n",
      "\n",
      "Agent: 15, mean return: 0.23\n",
      "Agent 0 performed worse than agent 8 but it wasn't significant, p = 0.34\n",
      "Overwriting agent 1 with agent 11, p = 0.01\n",
      "Overwriting agent 2 with agent 12, p = 0.00\n",
      "Overwriting agent 5 with agent 0, p = 0.04\n",
      "Overwriting agent 7 with agent 9, p = 0.00\n",
      "Overwriting agent 10 with agent 14, p = 0.00\n",
      "Overwriting agent 11 with agent 12, p = 0.00\n",
      "Overwriting agent 13 with agent 7, p = 0.00\n",
      "Overwriting agent 14 with agent 4, p = 0.00\n",
      "Overwriting agent 15 with agent 1, p = 0.02\n",
      "Episode 180\tAverage Score: 0.76\n",
      "\n",
      "Agent: 0, mean return: 1.00\n",
      "Episode 180\tAverage Score: 0.76\n",
      "\n",
      "Agent: 1, mean return: 0.77\n",
      "Episode 180\tAverage Score: 0.89\n",
      "\n",
      "Agent: 2, mean return: 3.20\n",
      "Episode 180\tAverage Score: 0.39\n",
      "\n",
      "Agent: 3, mean return: 0.70\n",
      "Episode 180\tAverage Score: 2.20\n",
      "\n",
      "Agent: 4, mean return: 3.47\n",
      "Episode 180\tAverage Score: 0.56\n",
      "\n",
      "Agent: 5, mean return: 1.09\n",
      "Episode 180\tAverage Score: 1.92\n",
      "\n",
      "Agent: 6, mean return: 4.37\n",
      "Episode 180\tAverage Score: 1.57\n",
      "\n",
      "Agent: 7, mean return: 5.24\n",
      "Episode 180\tAverage Score: 0.77\n",
      "\n",
      "Agent: 8, mean return: 1.04\n",
      "Episode 180\tAverage Score: 2.14\n",
      "\n",
      "Agent: 9, mean return: 5.95\n",
      "Episode 180\tAverage Score: 0.94\n",
      "\n",
      "Agent: 10, mean return: 2.08\n",
      "Episode 180\tAverage Score: 1.40\n",
      "\n",
      "Agent: 11, mean return: 4.57\n",
      "Episode 180\tAverage Score: 2.44\n",
      "\n",
      "Agent: 12, mean return: 5.10\n",
      "Episode 180\tAverage Score: 1.40\n",
      "\n",
      "Agent: 13, mean return: 5.05\n",
      "Episode 180\tAverage Score: 1.28\n",
      "\n",
      "Agent: 14, mean return: 4.33\n",
      "Episode 180\tAverage Score: 0.37\n",
      "\n",
      "Agent: 15, mean return: 0.50\n",
      "Overwriting agent 0 with agent 6, p = 0.00\n",
      "Overwriting agent 1 with agent 12, p = 0.00\n",
      "Overwriting agent 2 with agent 7, p = 0.04\n",
      "Overwriting agent 5 with agent 14, p = 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting agent 10 with agent 14, p = 0.00\n",
      "Overwriting agent 15 with agent 5, p = 0.02\n",
      "Episode 200\tAverage Score: 1.44\n",
      "\n",
      "Agent: 0, mean return: 4.18\n",
      "Episode 200\tAverage Score: 3.11\n",
      "\n",
      "Agent: 1, mean return: 12.37\n",
      "Episode 200\tAverage Score: 3.21\n",
      "\n",
      "Agent: 2, mean return: 11.66\n",
      "Episode 200\tAverage Score: 0.46\n",
      "\n",
      "Agent: 3, mean return: 0.49\n",
      "Episode 200\tAverage Score: 3.68\n",
      "\n",
      "Agent: 4, mean return: 9.51\n",
      "Episode 200\tAverage Score: 1.68\n",
      "\n",
      "Agent: 5, mean return: 5.73\n",
      "Episode 200\tAverage Score: 3.51\n",
      "\n",
      "Agent: 6, mean return: 8.59\n",
      "Episode 200\tAverage Score: 3.58\n",
      "\n",
      "Agent: 7, mean return: 10.50\n",
      "Episode 200\tAverage Score: 0.93\n",
      "\n",
      "Agent: 8, mean return: 1.14\n",
      "Episode 200\tAverage Score: 4.47\n",
      "\n",
      "Agent: 9, mean return: 11.84\n",
      "Episode 200\tAverage Score: 2.21\n",
      "\n",
      "Agent: 10, mean return: 6.91\n",
      "Episode 200\tAverage Score: 3.70\n",
      "\n",
      "Agent: 11, mean return: 11.71\n",
      "Episode 200\tAverage Score: 5.27\n",
      "\n",
      "Agent: 12, mean return: 14.71\n",
      "Episode 200\tAverage Score: 4.01\n",
      "\n",
      "Agent: 13, mean return: 13.45\n",
      "Episode 200\tAverage Score: 2.37\n",
      "\n",
      "Agent: 14, mean return: 5.83\n",
      "Episode 200\tAverage Score: 1.70\n",
      "\n",
      "Agent: 15, mean return: 7.07\n",
      "Overwriting agent 0 with agent 11, p = 0.00\n",
      "Overwriting agent 3 with agent 11, p = 0.00\n",
      "Agent 4 performed worse than agent 11 but it wasn't significant, p = 0.13\n",
      "Overwriting agent 8 with agent 2, p = 0.00\n",
      "Agent 10 performed worse than agent 6 but it wasn't significant, p = 0.23\n",
      "Overwriting agent 14 with agent 1, p = 0.00\n",
      "Episode 220\tAverage Score: 4.78\n",
      "\n",
      "Agent: 0, mean return: 17.22\n",
      "Episode 220\tAverage Score: 5.89\n",
      "\n",
      "Agent: 1, mean return: 15.05\n",
      "Episode 220\tAverage Score: 6.58\n",
      "\n",
      "Agent: 2, mean return: 17.00\n",
      "Episode 220\tAverage Score: 5.06\n",
      "\n",
      "Agent: 3, mean return: 23.23\n",
      "Episode 220\tAverage Score: 7.39\n",
      "\n",
      "Agent: 4, mean return: 19.98\n",
      "Episode 220\tAverage Score: 3.73\n",
      "\n",
      "Agent: 5, mean return: 10.41\n",
      "Episode 220\tAverage Score: 6.38\n",
      "\n",
      "Agent: 6, mean return: 15.23\n",
      "Episode 220\tAverage Score: 7.47\n",
      "\n",
      "Agent: 7, mean return: 19.66\n",
      "Episode 220\tAverage Score: 4.14\n",
      "\n",
      "Agent: 8, mean return: 16.69\n",
      "Episode 220\tAverage Score: 8.78\n",
      "\n",
      "Agent: 9, mean return: 22.25\n",
      "Episode 220\tAverage Score: 5.19\n",
      "\n",
      "Agent: 10, mean return: 15.94\n",
      "Episode 220\tAverage Score: 6.94\n",
      "\n",
      "Agent: 11, mean return: 16.69\n",
      "Episode 220\tAverage Score: 8.58\n",
      "\n",
      "Agent: 12, mean return: 18.31\n",
      "Episode 220\tAverage Score: 7.93\n",
      "\n",
      "Agent: 13, mean return: 20.31\n",
      "Episode 220\tAverage Score: 6.05\n",
      "\n",
      "Agent: 14, mean return: 18.52\n",
      "Episode 220\tAverage Score: 3.30\n",
      "\n",
      "Agent: 15, mean return: 8.40\n",
      "Overwriting agent 1 with agent 4, p = 0.02\n",
      "Agent 4 performed worse than agent 13 but it wasn't significant, p = 0.89\n",
      "Overwriting agent 5 with agent 8, p = 0.00\n",
      "Agent 6 performed worse than agent 0 but it wasn't significant, p = 0.27\n",
      "Agent 10 performed worse than agent 12 but it wasn't significant, p = 0.09\n",
      "Agent 11 performed worse than agent 14 but it wasn't significant, p = 0.32\n",
      "Agent 12 performed worse than agent 7 but it wasn't significant, p = 0.34\n",
      "Agent 13 performed worse than agent 3 but it wasn't significant, p = 0.27\n",
      "Overwriting agent 15 with agent 2, p = 0.00\n",
      "Episode 240\tAverage Score: 8.42\n",
      "\n",
      "Agent: 0, mean return: 18.84\n",
      "Episode 240\tAverage Score: 11.02\n",
      "\n",
      "Agent: 1, mean return: 26.49\n",
      "Episode 240\tAverage Score: 11.36\n",
      "\n",
      "Agent: 2, mean return: 24.20\n",
      "Episode 240\tAverage Score: 10.79\n",
      "\n",
      "Agent: 3, mean return: 28.80\n",
      "Episode 240\tAverage Score: 13.76\n",
      "\n",
      "Agent: 4, mean return: 33.31\n",
      "Episode 240\tAverage Score: 8.49\n",
      "\n",
      "Agent: 5, mean return: 24.73\n",
      "Episode 240\tAverage Score: 10.33\n",
      "\n",
      "Agent: 6, mean return: 21.28\n",
      "Episode 240\tAverage Score: 12.68\n",
      "\n",
      "Agent: 7, mean return: 26.85\n",
      "Episode 240\tAverage Score: 9.37\n",
      "\n",
      "Agent: 8, mean return: 26.79\n",
      "Episode 240\tAverage Score: 15.30\n",
      "\n",
      "Agent: 9, mean return: 33.37\n",
      "Episode 240\tAverage Score: 9.46\n",
      "\n",
      "Agent: 10, mean return: 22.06\n",
      "Episode 240\tAverage Score: 13.25\n",
      "\n",
      "Agent: 11, mean return: 32.41\n",
      "Episode 240\tAverage Score: 14.34\n",
      "\n",
      "Agent: 12, mean return: 30.31\n",
      "Episode 240\tAverage Score: 14.61\n",
      "\n",
      "Agent: 13, mean return: 33.84\n",
      "Episode 240\tAverage Score: 11.41\n",
      "\n",
      "Agent: 14, mean return: 27.10\n",
      "Episode 240\tAverage Score: 8.41\n",
      "\n",
      "Agent: 15, mean return: 25.83\n",
      "Overwriting agent 0 with agent 5, p = 0.00\n",
      "Overwriting agent 2 with agent 13, p = 0.00\n",
      "Overwriting agent 6 with agent 11, p = 0.00\n",
      "Agent 7 performed worse than agent 14 but it wasn't significant, p = 0.91\n",
      "Agent 8 performed worse than agent 3 but it wasn't significant, p = 0.38\n",
      "Agent 10 performed worse than agent 2 but it wasn't significant, p = 0.34\n",
      "Agent 15 performed worse than agent 3 but it wasn't significant, p = 0.18\n",
      "Episode 260\tAverage Score: 15.54\n",
      "\n",
      "Agent: 0, mean return: 36.45\n",
      "Episode 260\tAverage Score: 17.40\n",
      "\n",
      "Agent: 1, mean return: 32.32\n",
      "Episode 260\tAverage Score: 18.56\n",
      "\n",
      "Agent: 2, mean return: 36.77\n",
      "Episode 260\tAverage Score: 17.54\n",
      "\n",
      "Agent: 3, mean return: 34.48\n",
      "Episode 260\tAverage Score: 20.33\n",
      "\n",
      "Agent: 4, mean return: 35.36\n",
      "Episode 260\tAverage Score: 14.83\n",
      "\n",
      "Agent: 5, mean return: 32.19\n",
      "Episode 260\tAverage Score: 16.82\n",
      "\n",
      "Agent: 6, mean return: 34.62\n",
      "Episode 260\tAverage Score: 19.58\n",
      "\n",
      "Agent: 7, mean return: 35.63\n",
      "Episode 260\tAverage Score: 15.72\n",
      "\n",
      "Agent: 8, mean return: 32.95\n",
      "Episode 260\tAverage Score: 21.93\n",
      "\n",
      "Agent: 9, mean return: 36.24\n",
      "Episode 260\tAverage Score: 16.06\n",
      "\n",
      "Agent: 10, mean return: 33.32\n",
      "Episode 260\tAverage Score: 20.48\n",
      "\n",
      "Agent: 11, mean return: 37.02\n",
      "Episode 260\tAverage Score: 20.91\n",
      "\n",
      "Agent: 12, mean return: 36.11\n",
      "Episode 260\tAverage Score: 21.57\n",
      "\n",
      "Agent: 13, mean return: 35.21\n",
      "Episode 260\tAverage Score: 17.89\n",
      "\n",
      "Agent: 14, mean return: 33.65\n",
      "Episode 260\tAverage Score: 15.12\n",
      "\n",
      "Agent: 15, mean return: 33.79\n",
      "Agent 1 performed worse than agent 8 but it wasn't significant, p = 0.69\n",
      "Overwriting agent 3 with agent 11, p = 0.01\n",
      "Agent 4 performed worse than agent 12 but it wasn't significant, p = 0.39\n",
      "Agent 5 performed worse than agent 14 but it wasn't significant, p = 0.26\n",
      "Agent 6 performed worse than agent 2 but it wasn't significant, p = 0.08\n",
      "Overwriting agent 8 with agent 0, p = 0.00\n",
      "Agent 10 performed worse than agent 6 but it wasn't significant, p = 0.47\n",
      "Agent 13 performed worse than agent 2 but it wasn't significant, p = 0.06\n",
      "Agent 14 performed worse than agent 6 but it wasn't significant, p = 0.51\n",
      "Episode 280\tAverage Score: 22.52\n",
      "\n",
      "Agent: 0, mean return: 35.89\n",
      "Episode 280\tAverage Score: 24.77\n",
      "\n",
      "Agent: 1, mean return: 37.61\n",
      "Episode 280\tAverage Score: 24.84\n",
      "\n",
      "Agent: 2, mean return: 34.59\n",
      "Episode 280\tAverage Score: 24.50\n",
      "\n",
      "Agent: 3, mean return: 35.48\n",
      "Episode 280\tAverage Score: 26.42\n",
      "\n",
      "Agent: 4, mean return: 33.97\n",
      "Episode 280\tAverage Score: 21.21\n",
      "\n",
      "Agent: 5, mean return: 32.98\n",
      "Episode 280\tAverage Score: 22.95\n",
      "\n",
      "Agent: 6, mean return: 35.03\n",
      "Episode 280\tAverage Score: 25.82\n",
      "\n",
      "Agent: 7, mean return: 36.43\n",
      "Episode 280\tAverage Score: 22.81\n",
      "\n",
      "Agent: 8, mean return: 36.48\n",
      "Episode 280\tAverage Score: 28.01\n",
      "\n",
      "Agent: 9, mean return: 36.34\n",
      "Episode 280\tAverage Score: 22.59\n",
      "\n",
      "Agent: 10, mean return: 34.70\n",
      "Episode 280\tAverage Score: 26.44\n",
      "\n",
      "Agent: 11, mean return: 34.35\n",
      "Episode 280\tAverage Score: 26.71\n",
      "\n",
      "Agent: 12, mean return: 34.13\n",
      "Episode 280\tAverage Score: 27.42\n",
      "\n",
      "Agent: 13, mean return: 34.30\n",
      "Episode 280\tAverage Score: 24.05\n",
      "\n",
      "Agent: 14, mean return: 35.17\n",
      "Episode 280\tAverage Score: 21.85\n",
      "\n",
      "Agent: 15, mean return: 34.17\n",
      "Overwriting agent 2 with agent 9, p = 0.05\n",
      "Agent 4 performed worse than agent 0 but it wasn't significant, p = 0.12\n",
      "Agent 5 performed worse than agent 2 but it wasn't significant, p = 0.16\n",
      "Agent 6 performed worse than agent 14 but it wasn't significant, p = 0.90\n",
      "Overwriting agent 10 with agent 1, p = 0.04\n",
      "Overwriting agent 12 with agent 1, p = 0.00\n",
      "Agent 13 performed worse than agent 2 but it wasn't significant, p = 0.76\n",
      "Overwriting agent 15 with agent 1, p = 0.00\n",
      "Episode 300\tAverage Score: 28.93\n",
      "\n",
      "Agent: 0, mean return: 36.26\n",
      "Episode 300\tAverage Score: 29.37\n",
      "\n",
      "Agent: 1, mean return: 35.36\n",
      "Episode 300\tAverage Score: 29.35\n",
      "\n",
      "Agent: 2, mean return: 34.22\n",
      "Episode 297\tAverage Score: 30.28\n",
      "Environment solved in 197 episodes!\tAverage Score: 30.28\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DdpgAgent' object has no attribute 'full_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-61ace452287c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpopulation_based_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbrain_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexisting_scores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_episode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisodes_between_exploit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-b206a4fc798b>\u001b[0m in \u001b[0;36mpopulation_based_training\u001b[1;34m(agents, env, brain_name, max_episode, max_t, episodes_between_exploit, p_threshold, existing_scores, checkpoint_episodes)\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mprevious_scores\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magent_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent_i\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0magent_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"agent_{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                 \u001b[0mcheckpoint_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint_episodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             )\n\u001b[0;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\r\\nAgent: {}, mean return: {:.2f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent_i\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mepisodes_between_exploit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-17a74ace90be>\u001b[0m in \u001b[0;36mddpg\u001b[1;34m(agent, env, brain_name, max_episode, max_t, previous_scores, solution_threshold, solution_episodes, checkpoint_episodes, agent_name)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores_window\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0msolution_episodes\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores_window\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[0msolution_threshold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_episode\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0msolution_episodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores_window\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"{}_solved.pth\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DdpgAgent' object has no attribute 'full_state_dict'"
     ]
    }
   ],
   "source": [
    "scores = population_based_training(agents, env, brain_name, existing_scores=scores, max_episode=400, episodes_between_exploit=20, checkpoint_episodes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.42\n",
      "Episode 200\tAverage Score: 1.11\n"
     ]
    }
   ],
   "source": [
    "agent = DdpgAgent()\n",
    "scores = ddpg(agent, env, brain_name, max_episode=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d2d97b0f0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue with a longer run overnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300\tAverage Score: 1.33\n",
      "Episode 400\tAverage Score: 1.05\n",
      "Episode 456\tAverage Score: 0.64"
     ]
    }
   ],
   "source": [
    "cont_scores = ddpg(agent, env, brain_name, max_episode=2000, previous_scores=scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
